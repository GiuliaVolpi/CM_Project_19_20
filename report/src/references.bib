@article{Hestenes1952,
author = {Hestenes, Stiefel},
year = {1952},
month = {12},
pages = {409-436},
title = {Methods of Conjugate Gradients for Solving Linear Systems},
volume = {49},
journal = {Journal of Research of the National Bureau of Standards}
}

@article{Fletcher1964,
author = {Fletcher, Reeves},
year = {1964},
month = {7},
pages = {149-154},
title = {Function minimization by conjugate gradients},
journal = {Computer Journal}
}

@book{Nocedal,
    title = {Numerical Optimization},
    author = {Jorge Nocedal, Stephen J. Wright},
    series = {Springer Series in Operations Research},
    year = {1999},
    publisher = {Springer}
}


@misc{Dua:2019 ,
author = "Dua, Dheeru and Graff, Casey",
year = "2017",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences" }

@book{intel-alt,
    title    = {Intel Math Kernel Library. Reference Manual},
    publisher= {Intel Corporation},
    address  = {Santa Clara, USA},
    note     = {ISBN 630813-054US},
    year     = {2009},
}


@ARTICLE{2019arXiv190710121V,
       author = {{Virtanen}, Pauli and {Gommers}, Ralf and {Oliphant},
         Travis E.  and {Haberland}, Matt and {Reddy}, Tyler and
         {Cournapeau}, David and {Burovski}, Evgeni and {Peterson}, Pearu
         and {Weckesser}, Warren and {Bright}, Jonathan and {van der Walt},
         St{\'e}fan J.  and {Brett}, Matthew and {Wilson}, Joshua and
         {Jarrod Millman}, K.  and {Mayorov}, Nikolay and {Nelson}, Andrew
         R.~J. and {Jones}, Eric and {Kern}, Robert and {Larson}, Eric and
         {Carey}, CJ and {Polat}, {\.I}lhan and {Feng}, Yu and {Moore},
         Eric W. and {Vand erPlas}, Jake and {Laxalde}, Denis and
         {Perktold}, Josef and {Cimrman}, Robert and {Henriksen}, Ian and
         {Quintero}, E.~A. and {Harris}, Charles R and {Archibald}, Anne M.
         and {Ribeiro}, Ant{\^o}nio H. and {Pedregosa}, Fabian and
         {van Mulbregt}, Paul and {Contributors}, SciPy 1. 0},
        title = "{SciPy 1.0--Fundamental Algorithms for Scientific
                  Computing in Python}",
      journal = {arXiv e-prints},
         year = "2019",
        month = "Jul",
          eid = {arXiv:1907.10121},
        pages = {arXiv:1907.10121},
archivePrefix = {arXiv},
       eprint = {1907.10121},
 primaryClass = {cs.MS},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190710121V},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@Inbook{Prechelt2012,
author="Prechelt, Lutz",
editor="Montavon, Gr{\'e}goire
and Orr, Genevi{\`e}ve B.
and M{\"u}ller, Klaus-Robert",
title="Early Stopping --- But When?",
bookTitle="Neural Networks: Tricks of the Trade: Second Edition",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="53--67",
abstract="Validation can be used to detect when overfitting starts during supervised training of a neural network; training is then stopped before convergence to avoid the overfitting (``early stopping''). The exact criterion used for validation-based early stopping, however, is usually chosen in an ad-hoc fashion or training is stopped interactively. This trick describes how to select a stopping criterion in a systematic fashion; it is a trick for either speeding learning procedures or improving generalization, whichever is more important in the particular situation. An empirical investigation on multi-layer perceptrons shows that there exists a tradeoff between training time and generalization: From the given mix of 1296 training runs using different 12 problems and 24 different network architectures I conclude slower stopping criteria allow for small improvements in generalization (here: about 4{\%} on average), but cost much more training time (here: about factor 4 longer on average).",
isbn="978-3-642-35289-8",
doi="10.1007/978-3-642-35289-8_5",
url="https://doi.org/10.1007/978-3-642-35289-8_5"
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@misc{nakerst2020gradient,
    title={Gradient descent with momentum --- to accelerate or to super-accelerate?},
    author={Goran Nakerst and John Brennan and Masudul Haque},
    year={2020},
    eprint={2001.06472},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{Polyak1964,
author = {Polyak, Boris},
year = {1964},
month = {12},
pages = {1-17},
title = {Some methods of speeding up the convergence of iteration methods},
volume = {4},
journal = {Ussr Computational Mathematics and Mathematical Physics},
doi = {10.1016/0041-5553(64)90137-5}
}

@article{sutskever2013,
author = {Sutskever, I. and Martens, J. and Dahl, G. and Hinton, G.},
year = {2013},
month = {01},
pages = {1139-1147},
title = {On the importance of initialization and momentum in deep learning},
journal = {30th International Conference on Machine Learning, ICML 2013}
}

@INPROCEEDINGS{Krizhevsky_imagenetclassification,
    author = {Alex Krizhevsky and Ilya Sutskever and Geoffrey E. Hinton},
    title = {Imagenet classification with deep convolutional neural networks},
    booktitle = {Advances in Neural Information Processing Systems},
    pages = {2012}
}

@INPROCEEDINGS{Glorot10understandingthe,
    author = {Xavier Glorot and Yoshua Bengio},
    title = {Understanding the difficulty of training deep feedforward neural networks},
    booktitle = {In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATSâ€™10). Society for Artificial Intelligence and Statistics},
    year = {2010}
}


@book{haykin2009neural,
  abstract = {Neural Networks and Learning Machines, Third Edition is renowned for its thoroughness and readability. This well-organized and completely up-to-date text remains the most comprehensive treatment of neural networks from an engineering perspective. This is ideal for professional engineers and research scientists.
 
Matlab codes used for the computer experiments in the text are available for download at: http://www.pearsonhighered.com/haykin/
 
Refocused, revised and renamed to reflect the duality of neural networks and learning machines, this edition recognizes that the subject matter is richer when these topics are studied together. Ideas drawn from neural networks and machine learning are hybridized to perform improved learning tasks beyond the capability of either independently.},
  added-at = {2017-03-18T17:31:57.000+0100},
  address = {Upper Saddle River, NJ},
  author = {Haykin, Simon S.},
  biburl = {https://www.bibsonomy.org/bibtex/2e5015812328aaeccd73d8b03a7e36831/vngudivada},
  edition = {Third},
  interhash = {4cef19efafc52ae42607f9832a205214},
  intrahash = {e5015812328aaeccd73d8b03a7e36831},
  keywords = {Book Learning NeuralNetwork},
  publisher = {Pearson Education},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Neural networks and learning machines},
  year = 2009
}

@article{DBLP:journals/corr/BadrinarayananK15,
  author    = {Vijay Badrinarayanan and
               Alex Kendall and
               Roberto Cipolla},
  title     = {SegNet: {A} Deep Convolutional Encoder-Decoder Architecture for Image
               Segmentation},
  journal   = {CoRR},
  volume    = {abs/1511.00561},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.00561},
  archivePrefix = {arXiv},
  eprint    = {1511.00561},
  timestamp = {Mon, 13 Aug 2018 16:46:06 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BadrinarayananK15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{Mitchell97,
  abstract = {This book covers the field of machine learning, which is the study of algorithms that allow computer programs to automatically improve through experience. This exciting addition to the McGraw-Hill Series in Computer Science focuses on the concepts and techniques that contribute to the rapidly changing field of machine learning---including probability and statistics, artificial intelligence, and neural networks---unifying them all in a logical and coherent manner.},
  added-at = {2017-05-08T14:37:30.000+0200},
  address = {New York},
  author = {Mitchell, Tom M.},
  biburl = {https://www.bibsonomy.org/bibtex/23e79734ee1a6e49aee02ffd108224d1c/flint63},
  file = {eBook:1900-99/Mitchell97.pdf:PDF;McGraw-Hill Product page:http\://www.mhprofessional.com/product.php?isbn=0070428077:URL;Amazon Search inside:http\://www.amazon.de/gp/reader/0070428077/:URL},
  groups = {public},
  interhash = {479a66c32badb3a455fbdcf8e6633a5d},
  intrahash = {3e79734ee1a6e49aee02ffd108224d1c},
  isbn = {978-0-07-042807-2},
  keywords = {01624 105 book shelf ai learn algorithm},
  publisher = {McGraw-Hill},
  timestamp = {2017-07-13T17:10:10.000+0200},
  title = {Machine Learning},
  username = {flint63},
  year = 1997
}


@article{10029946121,
author="NESTEROV, Y. E.",
title="A method for solving the convex programming problem with convergence rate O($1/k^2$)",
journal="Dokl. Akad. Nauk SSSR",
ISSN="",
publisher="",
year="1983",
month="",
volume="269",
number="",
pages="543-547",
URL="https://ci.nii.ac.jp/naid/10029946121/en/",
DOI="",
}

@article{baali,
author = {Al-Baali, Mehiddin},
year = {1985},
month = {01},
pages = {},
title = {Descent Property and Global Convergence of the Fletcher--Reeves Method with Inexact Line Search},
volume = {5},
journal = {IMA Journal of Numerical Analysis},
doi = {10.1093/imanum/5.1.121}
}

@article{nocedal_global_convergence,
author = {Gilbert, Jean Charles and Nocedal, Jorge},
year = {1992},
month = {02},
pages = {21-42},
title = {Global Convergence Properties of Conjugate Gradient Methods for Optimization},
volume = {2},
journal = {SIAM Journal on Optimization},
doi = {10.1137/0802003}
}

@InProceedings{powell,
author="Powell, M. J. D.",
editor="Griffiths, David F.",
title="Nonconvex minimization calculations and the conjugate gradient method",
booktitle="Numerical Analysis",
year="1984",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="122--141"
}

@article{davidon,
title = {VARIABLE METRIC METHOD FOR MINIMIZATION},
author = {Davidon, W C},
doi = {10.2172/4252678},
place = {United States},
year = {1959},
month = {5}
}

@book{doi:10.1137/1.9781611971200,
author = {Dennis, J. E. and Schnabel, Robert B.},
title = {Numerical Methods for Unconstrained Optimization and Nonlinear Equations},
publisher = {Society for Industrial and Applied Mathematics},
year = {1996},
doi = {10.1137/1.9781611971200},
address = {},
edition   = {},
URL = {https://epubs.siam.org/doi/abs/10.1137/1.9781611971200},
eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611971200}
}

@article{10.1093/comjnl/6.2.163,
    author = {Fletcher, R. and Powell, M. J. D.},
    title = "{A Rapidly Convergent Descent Method for Minimization}",
    journal = {The Computer Journal},
    volume = {6},
    number = {2},
    pages = {163-168},
    year = {1963},
    month = {08},
    abstract = "{A powerful iterative descent method for finding a local minimum of a function of several variables is described. A number of theorems are proved to show that it always converges and that it converges rapidly. Numerical tests on a variety of functions confirm these theorems. The method has been used to solve a system of one hundred non-linear simultaneous equations.}",
    issn = {0010-4620},
    doi = {10.1093/comjnl/6.2.163},
    url = {https://doi.org/10.1093/comjnl/6.2.163},
    eprint = {https://academic.oup.com/comjnl/article-pdf/6/2/163/1041527/6-2-163.pdf},
}

@ARTICLE{Liu,
    author = {Dong C. Liu and Jorge Nocedal},
    title = {On the Limited Memory BFGS Method for Large Scale Optimization},
    journal = {MATHEMATICAL PROGRAMMING},
    year = {1989},
    volume = {45},
    pages = {503--528}
}

@Article{Powell1977,
author={Powell, M. J. D.},
title={Restart procedures for the conjugate gradient method},
journal={Mathematical Programming},
year={1977},
month={Dec},
day={01},
volume={12},
number={1},
pages={241-254},
issn={1436-4646},
doi={10.1007/BF01593790},
url={https://doi.org/10.1007/BF01593790}
}

 @comment{ G. Zoutendijk, Nonlinear Programming, Computational Methods, In: J. 
           Abadie Ed., Integer and Nonlinear Programming, North-Holland, 
           Amsterdam, 1970, pp. 37-86. }
@book{zoutendijk1970northholland,
 author = "G. Zoutendijk and Nonlinear Programming and Computational Methods and In: J. Abadie Ed. and Integer and Nonlinear Programming",
 title = "North-Holland, Amsterdam",
 pages = "37--86",
 year = 1970
}


@ARTICLE{Crowder_rate,  author={H. {Crowder} and P. {Wolfe}},  journal={IBM Journal of Research and Development},   title={Linear Convergence of the Conjugate Gradient Method},   year={1972},  volume={16},  number={4},  pages={431-433},}

@article{powell_rate_conv,
author = {Powell, M. J.},
title = {Some Convergence Properties of the Conjugate Gradient Method},
year = {1976},
issue_date = {December  1976},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {11},
number = {1},
issn = {0025-5610},
url = {https://doi.org/10.1007/BF01580369},
doi = {10.1007/BF01580369},
journal = {Math. Program.},
month = dec,
pages = {42â€“49},
numpages = {8}
}

@article{Cohen_rate_conv,
 ISSN = {00361429},
 URL = {http://www.jstor.org/stable/2156398},
 author = {Arthur I. Cohen},
 journal = {SIAM Journal on Numerical Analysis},
 number = {2},
 pages = {248--259},
 publisher = {Society for Industrial and Applied Mathematics},
 title = {Rate of Convergence of Several Conjugate Gradient Algorithms},
 volume = {9},
 year = {1972}
}

@article{Fukushima,
title = "A modified BFGS method and its global convergence in nonconvex minimization",
journal = "Journal of Computational and Applied Mathematics",
volume = "129",
number = "1",
pages = "15 - 35",
year = "2001",
note = "Nonlinear Programming and Variational Inequalities",
issn = "0377-0427",
doi = "https://doi.org/10.1016/S0377-0427(00)00540-9",
url = "http://www.sciencedirect.com/science/article/pii/S0377042700005409",
author = "Dong-Hui Li and Masao Fukushima",
keywords = "BFGS method, Global convergence, Superlinear convergence, Nonconvex minimization",
abstract = "In this paper, we propose a modification of the BFGS method for unconstrained optimization. A remarkable feature of the proposed method is that it possesses a global convergence property even without convexity assumption on the objective function. Under certain conditions, we also establish superlinear convergence of the method."
}

@techreport{Dennis-More,
author = {Dennis, John E. and More, Jorge J.},
title = {Quasi-Newton Methods, Motivation and Theory},
year = {1974},
publisher = {Cornell University},
address = {USA},
abstract = {This paper is an attempt to motivate and justify quasi-Newton methods as useful modifications of Newton''s method for general and gradient nonlinear systems of equations. References are given to ample numerical justification; here we give an overview of many of the important theoretical results and each is accompanied by sufficient discussion to make the results and hence the methods plausible. Key Words and Phrases: unconstrained minimization, nonlinear simultaneous equations, update methods, quasi-Newton methods.}
}

@book{Bazaraa_nonlinear,
author = {Bazaraa, Mokhtar S.},
title = {Nonlinear Programming: Theory and Algorithms},
year = {2013},
isbn = {1118857569},
publisher = {Wiley Publishing},
edition = {3rd}
}

@article{Zangwill_nonlinear,
author = {Zangwill, Willard I.},
title = {Convergence Conditions for Nonlinear Programming Algorithms},
journal = {Management Science},
volume = {16},
number = {1},
pages = {1-13},
year = {1969},
doi = {10.1287/mnsc.16.1.1}
}

@article{nocedal_global_c_quasi_newton,
 ISSN = {00361429},
 URL = {http://www.jstor.org/stable/2157680},
 author = {Richard H. Byrd and Jorge Nocedal},
 journal = {SIAM Journal on Numerical Analysis},
 number = {3},
 pages = {727--739},
 publisher = {Society for Industrial and Applied Mathematics},
 title = {A Tool for the Analysis of Quasi-Newton Methods with Application to Unconstrained Minimization},
 volume = {26},
 year = {1989}
}

@article{nocedal_global_c_quasi_newton_2,
 ISSN = {00361429},
 URL = {http://www.jstor.org/stable/2157646},
 journal = {SIAM Journal on Numerical Analysis},
 number = {5},
 pages = {1171--1190},
 publisher = {Society for Industrial and Applied Mathematics},
 title = {Global Convergence of a Class of Quasi-Newton Methods on Convex Problems},
 volume = {24},
 year = {1987}
}

@inproceedings{Powell1976SomeGC,
  title={Some global convergence properties of a variable metric algorithm for minimization without exact lin},
  author={M. Powell},
  year={1976}
}

@article{dai_bfgs_controes,
author = {Dai, Yu-Hong},
year = {2002},
month = {01},
pages = {693-701},
title = {Convergence Properties of the BFGS Algoritm},
volume = {13},
journal = {SIAM Journal on Optimization},
doi = {10.1137/S1052623401383455}
}



